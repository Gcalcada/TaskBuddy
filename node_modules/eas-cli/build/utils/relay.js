"use strict";
Object.defineProperty(exports, "__esModule", { value: true });
exports.NEXT_PAGE_OPTION = exports.PREV_PAGE_OPTION = exports.selectPaginatedAsync = exports.FilterPagination = void 0;
const tslib_1 = require("tslib");
const assert_1 = tslib_1.__importDefault(require("assert"));
const prompts_1 = require("../prompts");
/**
 *
 * Pagination that performs client side filtering on the nodes returned from a relay compliant datasource.
 *
 * @param queryParams The query params for the pagination.
 * @param queryAsync A promise based function for querying.
 * @param filterPredicate A predicate function to filter the node.
 * @param beforeEachQuery Optional. A callback function to be called before each query
 * @param afterEachQuery Optional. A callback function to be called after each query.
 * @param internalBatchSize Optional. The batch size of queryAsync. Defaults to 100.
 * @param maxNodesFetched Optional. The maximum number of nodes to fetch. Defaults to 10_000.
 * @param beforeEachQuery Optional. A callback function to be called before each query
 * @args externalQueryParams The query params for the pagination.
 * @args totalNodesFetched The total number of nodes fetched so far.
 * @args dataset The dataset so far.
 * @param afterEachQuery Optional. A callback function to be called after each query.
 * @args externalQueryParams The query params for the pagination.
 * @args totalNodesFetched The total number of nodes fetched so far.
 * @args dataset The dataset so far.
 * @args willFetchAgain If the query will fetch again to get a complete page.
 *
 * @throws {Error} - If an error occurs during execution of the query or pagination.
 */
class FilterPagination {
    static async getPageAsync({ queryParams, queryAsync, filterPredicate, internalBatchSize = 100, maxNodesFetched = 10000, beforeEachQuery, afterEachQuery, }) {
        if (this.isFirstAfter(queryParams)) {
            return await this.getFirstItemsAsync(queryParams, {
                queryAsync,
                filterPredicate,
                internalBatchSize,
                maxNodesFetched,
                beforeEachQuery,
                afterEachQuery,
            });
        }
        else if (this.isLastBefore(queryParams)) {
            return await this.getLastItemsAsync(queryParams, {
                queryAsync,
                filterPredicate,
                internalBatchSize,
                maxNodesFetched,
                beforeEachQuery,
                afterEachQuery,
            });
        }
        throw new Error('Invalid query params');
    }
    static isFirstAfter(connectionArgs) {
        return 'first' in connectionArgs;
    }
    static isLastBefore(connectionArgs) {
        return 'last' in connectionArgs;
    }
    static async getFirstItemsAsync({ first, after }, { internalBatchSize, maxNodesFetched, filterPredicate, queryAsync, beforeEachQuery, afterEachQuery, }) {
        var _a, _b, _c, _d, _e;
        const limit = first + 1;
        const dataset = [];
        let hasMore = true;
        let afterInternal = after;
        let totalNodesFetched = 0;
        while (hasMore && dataset.length < limit) {
            if (beforeEachQuery) {
                beforeEachQuery({ first, after }, totalNodesFetched, dataset);
            }
            const result = await queryAsync({ first: internalBatchSize, after: afterInternal });
            const { edges: batchEdges, pageInfo } = result;
            const batch = batchEdges.filter(edge => filterPredicate(edge.node));
            const nodesRemaining = limit - dataset.length;
            dataset.push(...batch.slice(0, nodesRemaining));
            hasMore = pageInfo.hasNextPage;
            afterInternal = (_a = pageInfo.endCursor) !== null && _a !== void 0 ? _a : undefined;
            totalNodesFetched += batchEdges.length;
            if (afterEachQuery) {
                afterEachQuery({ first, after }, totalNodesFetched, dataset, hasMore && dataset.length < limit);
            }
            if (totalNodesFetched >= maxNodesFetched) {
                throw new Error(`Max nodes of ${maxNodesFetched} fetched`);
            }
        }
        const edges = dataset.slice(0, first);
        return {
            edges,
            pageInfo: {
                hasNextPage: dataset.length > first,
                hasPreviousPage: false, // cannot be computed efficiently
                startCursor: (_c = (_b = edges[0]) === null || _b === void 0 ? void 0 : _b.cursor) !== null && _c !== void 0 ? _c : null,
                endCursor: (_e = (_d = edges[edges.length - 1]) === null || _d === void 0 ? void 0 : _d.cursor) !== null && _e !== void 0 ? _e : null,
            },
        };
    }
    static async getLastItemsAsync({ last, before }, { internalBatchSize, maxNodesFetched, filterPredicate, queryAsync, beforeEachQuery, afterEachQuery, }) {
        var _a, _b, _c, _d, _e;
        const limit = last + 1;
        const dataset = [];
        let hasMore = true;
        let beforeInternal = before;
        let totalNodesFetched = 0;
        while (hasMore && dataset.length < limit) {
            if (beforeEachQuery) {
                beforeEachQuery({ last, before }, totalNodesFetched, dataset);
            }
            const result = await queryAsync({ last: internalBatchSize, before: beforeInternal });
            const { edges: batchEdges, pageInfo } = result;
            const batch = batchEdges.filter(edge => filterPredicate(edge.node));
            const nodesRemaining = limit - dataset.length;
            // relay orders pages from first to last, so we reverse the batch to to choose the last n
            const nodesChosen = batch.reverse().slice(0, nodesRemaining);
            dataset.push(...nodesChosen);
            hasMore = pageInfo.hasPreviousPage;
            beforeInternal = (_a = pageInfo.startCursor) !== null && _a !== void 0 ? _a : undefined;
            totalNodesFetched += batchEdges.length;
            if (afterEachQuery) {
                afterEachQuery({ last, before }, totalNodesFetched, dataset, hasMore && dataset.length < limit);
            }
            if (totalNodesFetched >= maxNodesFetched) {
                throw new Error(`Max nodes of ${maxNodesFetched} fetched`);
            }
        }
        // we reverse our dataset again to restore the original order of first to last to match relay
        const edges = dataset.slice(0, last).reverse();
        return {
            edges,
            pageInfo: {
                hasNextPage: false, // cannot be computed efficiently,
                hasPreviousPage: dataset.length > last,
                startCursor: (_c = (_b = edges[0]) === null || _b === void 0 ? void 0 : _b.cursor) !== null && _c !== void 0 ? _c : null,
                endCursor: (_e = (_d = edges[edges.length - 1]) === null || _d === void 0 ? void 0 : _d.cursor) !== null && _e !== void 0 ? _e : null,
            },
        };
    }
}
exports.FilterPagination = FilterPagination;
async function selectPaginatedAsync({ queryAsync, getTitleAsync, printedType, pageSize, }) {
    return await selectPaginatedInternalAsync({
        queryAsync,
        getTitleAsync,
        printedType,
        queryParams: { first: pageSize },
    });
}
exports.selectPaginatedAsync = selectPaginatedAsync;
exports.PREV_PAGE_OPTION = {
    value: Symbol('PREV_PAGE'),
    title: '⬆️ Previous page',
};
exports.NEXT_PAGE_OPTION = {
    value: Symbol('NEXT_PAGE'),
    title: '⬇️ Next page',
};
async function selectPaginatedInternalAsync({ queryAsync, getTitleAsync, printedType, queryParams, }) {
    var _a;
    const limit = (_a = queryParams.first) !== null && _a !== void 0 ? _a : queryParams.last;
    (0, assert_1.default)(limit, 'queryParams must have either first or last');
    const connection = await queryAsync(queryParams);
    const { edges, pageInfo } = connection;
    if (edges.length === 0) {
        return null;
    }
    /*
     * The Relay spec has a weird definition on hasNextPage and hasPreviousPage:
     * 'If the client is paginating with last/before, then the server must return true if prior edges
     * exist, otherwise false. If the client is paginating with first/after, then the client may
     * return true if edges prior to after exist, if it can do so efficiently, otherwise may return false.'
     *
     * This means if we are paginating with first/after, we can't rely on pageInfo.hasPreviousPage and vice versa.
     */
    const { endCursor, hasNextPage: serverResponseHasNextPage, startCursor, hasPreviousPage: serverResponseHasPreviousPage, } = pageInfo;
    const hasPreviousPage = serverResponseHasPreviousPage || queryParams.after;
    const hasNextPage = serverResponseHasNextPage || queryParams.before;
    const nodes = edges.map(edge => edge.node);
    const options = [];
    if (hasPreviousPage) {
        options.push(exports.PREV_PAGE_OPTION);
    }
    const nodeTitles = await Promise.all(nodes.map(node => getTitleAsync(node)));
    options.push(...nodes.map((node, index) => ({ value: node, title: nodeTitles[index] })));
    if (hasNextPage) {
        options.push(exports.NEXT_PAGE_OPTION);
    }
    const { item: selectedItem } = await (0, prompts_1.promptAsync)({
        type: 'select',
        name: 'item',
        message: `Select a ${printedType}`,
        choices: options.map(option => ({
            value: option.value,
            title: option.title,
        })),
    });
    if (selectedItem === exports.PREV_PAGE_OPTION.value) {
        return await selectPaginatedInternalAsync({
            queryParams: {
                last: limit,
                before: startCursor !== null && startCursor !== void 0 ? startCursor : undefined,
            },
            queryAsync,
            getTitleAsync,
            printedType,
        });
    }
    else if (selectedItem === exports.NEXT_PAGE_OPTION.value) {
        return await selectPaginatedInternalAsync({
            queryParams: {
                first: limit,
                after: endCursor !== null && endCursor !== void 0 ? endCursor : undefined,
            },
            queryAsync,
            getTitleAsync,
            printedType,
        });
    }
    else {
        return selectedItem;
    }
}
